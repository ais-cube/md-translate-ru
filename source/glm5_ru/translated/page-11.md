Кроме того, GLM-5 расширяет максимальную длину контекста до 202 752 токенов во время SFT. Наряду с обновлённым шаблоном чата модель поддерживает три различные характеристики рассуждения (см. рисунок 7), включая:
• Чередующееся рассуждение: модель рассуждает перед каждым ответом и вызовом инструмента, улучшая следование инструкциям и качество генерации4.
• Сохранённое рассуждение: в сценариях агента для написания кода модель автоматически сохраняет все блоки рассуждения в многошаговых диалогах, повторно используя существующие рассуждения вместо их повторного вывода с нуля. Это уменьшает потерю информации и несогласованности и хорошо подходит для долгосрочных сложных задач5.
• Рассуждение на уровне шага: модель поддерживает управление рассуждением для каждого шага в рамках сеанса — отключение рассуждения для простых запросов для снижения задержки/стоимости, включение для сложных задач для повышения точности и стабильности.
Рассуждая между действиями и поддерживая согласованность между шагами, GLM-5 достигает более стабильного и контролируемого поведения на сложных задачах.

Для общего чата мы оптимизируем стиль ответов, чтобы он был более логичным и лаконичным по сравнению с GLM-4.5. Для задач ролевой игры мы собираем и создаём более широкий и разнообразный набор данных, охватывающий несколько языков и конфигураций ролей. В частности, мы определяем несколько измерений оценки — включая следование инструкциям, языковую выразительность, креативность, логическую согласованность и согласованность в длинных диалогах — и применяем автоматическую и ручную фильтрацию для отбора и улучшения данных.

Для задач рассуждения мы дополнительно усиливаем глубину рассуждения модели. В частности, для логического рассуждения мы создаём проверяемые задачи и синтезируем высококачественные данные с использованием отбраковочной выборки. Для математических и научных задач применяется процесс фильтрации на основе сложности, сохраняя только задачи, которые являются сложными для модели GLM-4.7.

Для задач написания кода и агентов, по сравнению с GLM-4.5, GLM-5 создаёт большое количество сред выполнения для получения высококачественных траекторий с особым акцентом на реальные сценарии и долгосрочные задачи. Мы дополнительно улучшаем данные SFT, используя экспертное обучение с подкреплением и отбраковочную выборку. Ошибочные сегменты в траекториях сохраняются, но исключаются из функции потерь, что позволяет модели изучать поведение исправления ошибок без усиления неправильных действий.

3.2 RL для рассуждения

Базовый алгоритм RL. Наш алгоритм RL основан на GRPO [40] и включает технику IcePop [61] для смягчения несоответствия обучения-инференса, т.е. расхождения между распределением инференса и распределением обучения во время оптимизации RL. Мы явно различаем обучающую политику πtrain, используемую для обновления градиентов, и политику инференса πinfer, используемую для выборки траекторий. По сравнению с исходной формулировкой IcePop мы удаляем член KL-регуляризации для ускорения улучшения RL. Итоговая функция потерь оптимизации:
L(θ) = −Ex∼D,{yi}G
i=1∼πinfer
θold(·|x)
"
1
G
G
X
i=1
1
|yi|
|yi|
X
t=1
pop(ρi,t, 1/β, β)
· min

ri,t ˆAi,t, clip(ri,t, 1 −ϵlow, 1 + ϵhigh) ˆAi,t
 #
,
(1)
где коэффициент несоответствия обучения-инференса определяется как
ρi,t = πtrain
θold (yi,t | x, yi,<t)
πinfer
θold (yi,t | x, yi,<t).
4Чередующееся рассуждение впервые представлено https://platform.claude.com/docs/en/build-wit
h-claude/extended-thinking#interleaved-thinking
5Сохранённое рассуждение также было принято Claude начиная с Opus 4.5. См. https://platform.claude.com/
docs/en/build-with-claude/extended-thinking#thinking-block-preservation-in-claude-o
pus-4-5-and-later
11