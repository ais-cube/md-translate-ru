Список литературы
[1] Anthropic. System card: Claude opus 4.5, 2025.
[2] S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, P. Cameron, M. Jaggi, D. Alistarh, T. Hoefler,
and J. Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024.
[3] A. Backlund and L. Petersson. Vending-bench: A benchmark for long-term coherence of
autonomous agents. arXiv preprint arXiv:2502.15840, 2025.
[4] I. Badertdinov, A. Golubev, M. Nekrashevich, A. Shevtsov, S. Karasik, A. Andriushchenko,
M. Trofimova, D. Litvintseva, and B. Yangel. Swe-rebench: An automated pipeline for task
collection and decontaminated evaluation of software engineering agents. arXiv preprint
arXiv:2505.20411, 2025.
[5] Y. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, and
J. Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context
multitasks. In ACL'25, pages 3639–3664, 2025.
[6] C. Bandi, B. Hertzberg, G. Boo, T. Polakam, J. Da, S. Hassaan, M. Sharma, A. Park, E. Hernan-
dez, D. Rambado, et al. Mcp-atlas: A large-scale benchmark for tool-use competency with real
mcp servers. arXiv preprint arXiv:2602.00933, 2026.
[7] V. Barres, H. Dong, S. Ray, X. Si, and K. Narasimhan. τ 2-bench: Evaluating conversational
agents in a dual-control environment. arXiv preprint arXiv:2506.07982, 2025.
[8] G. DeepMind. Gemini 3 pro model card, 2025.
[9] DeepSeek-AI, A. Liu, A. Mei, and et al. Deepseek-v3.2: Pushing the frontier of open large
language models, 2025.
[10] W. Du, S. Toshniwal, B. Kisacanin, S. Mahdavi, I. Moshkov, G. Armstrong, S. Ge, E. Minasyan,
F. Chen, and I. Gitman. Nemotron-math: Efficient long-context distillation of mathematical
reasoning from multi-mode supervision. arXiv preprint arXiv:2512.15489, 2025.
[11] C. Gao, X. Wu, Z. Lin, D. Zhang, and S. Hu. Nextlong: Toward effective long-context training
without long documents, 2025.
[12] H. Ge, J. Feng, Q. Huang, F. Fu, X. Nie, L. Zuo, H. Lin, B. Cui, and X. Liu. Bytescale: Efficient
scaling of llm training with a 2048k context length on more than 12,000 gpus. arXiv preprint
arXiv:2502.21231, 2025.
[13] F. Gloeckle, B. Y. Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve. Better & faster large
language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.
[14] Y. Gu, L. Dong, F. Wei, and M. Huang. Minillm: Knowledge distillation of large language
models. In ICLR'23, 2025.
[15] Y. Gu, Q. Hu, S. Yang, H. Xi, J. Chen, S. Han, and H. Cai. Jet-nemotron: Efficient language
model with post neural architecture search. arXiv preprint arXiv:2508.15884, 2025.
[16] Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, Z. Lin,
X. Liu, D. Sun, S. Lin, Z. Zheng, X. Zhu, W. Su, and B. Zheng. Chinese simpleqa: A chinese
factuality evaluation for large language models, 2024.
[17] C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, and B. Ginsburg. Ruler: What's
the real context size of your long-context language models? In COLM'24, 2024.
[18] J. Jia, Z. Chen, X. Wu, C. Gao, Z. Lin, D. Zhang, S. Hu, and B. Guo. Entropylong: Effective
long-context training via predictive uncertainty, 2025.
[19] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench:
Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.
[20] Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative
decoding. In ICML'23, pages 19274–19286, 2023.
32