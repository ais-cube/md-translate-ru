NPU перекрывать вычисления с доступом к памяти. Для ядра Sparse Flash Attention мы
специально оптимизировали под паттерны разреженности GLM-5. Это ядро обрабатывает выбор TopK токенов
из KV-кэша и вычисление разреженного внимания параллельно. Наконец, MLAPO объединяет 13 небольших
операторов предобработки в один «супероператор», используя параллельную обработку между блоками Vector и
Cube для повышения сквозной эффективности.

Оптимизации специализированного движка вывода.
Мы адаптировали два ведущих движка вывода, vLLM-
Ascend и SGLang, для максимизации использования аппаратных ресурсов:
• Асинхронное планирование: В vLLM мы реализовали механизм перекрытия копирования семплирования
«Device-to-Host» (D2H) с подготовкой следующего шага декодирования, эффективно устраняя «пузыри»
планирования.
• Управление контекстом: Функции, такие как RadixCache (совместное использование префиксов) и Prefix Cache
(расширение хранилища KV в системную RAM), обеспечивают эффективное повторное использование
записей KV, что критично для производительности на длинных контекстах.
• Стратегия параллелизма: Мы использовали гибридный подход, сочетающий параллелизм данных внимания (DP)
и параллелизм экспертов MoE (EP), наряду с FlashComm, который разделяет операции AllReduce, чтобы
скрыть задержку коммуникации за вычислениями.
• Многотокенное предсказание (MTP): Генерируя несколько токенов за один шаг вывода, мы значительно
увеличили плотность вычислений NPU и сократили общее время генерации последовательности.

Благодаря этим аппаратным ко-оптимизациям GLM-5 на одном китайском узле достигает производительности,
сравнимой с двухпроцессорными международными кластерами, при этом снижая затраты на развёртывание
в сценариях с длинными последовательностями на 50 %.

6
Оценка

Как показано выше, GLM-5 знаменует переход от vibe coding к новой эре агентной инженерии.
Сначала мы оцениваем GLM-5 с передовыми моделями на агентных, рассуждающих и кодирующих (ARC) бенчмарках.
Для полной оценки производительности GLM-5 в реальных сценариях агентной инженерии мы предлагаем
новый внутренний набор для оценки, CC-Bench-V2, который включает задачи фронтенда, бэкенда и долгосрочные
задачи. Наконец, мы оцениваем общие способности GLM-5 в пяти распространённых реальных сценариях.

6.1
Оценка ARC-бенчмарков

Мы представляем основные результаты ARC-бенчмарков в таблице 7, которые сравнивают GLM-5 с GLM-
4.7, DeepSeek-V3.2 [26], Kimi-K2.5 [43], Claude Opus 4.5 [1], Gemini 3 Pro [8] и GPT-5.2
(xhigh) [32]. В целом, GLM-5 демонстрирует значительное улучшение по сравнению с GLM-4.7 и достигает
современной производительности среди открытых моделей, сокращая разрыв с проприетарными моделями,
такими как Claude Opus 4.5. Детали оценки можно найти в разделе B.2.

6.1.1
Оценка бенчмарков рассуждений и общих бенчмарков

Для бенчмарков рассуждений и общих бенчмарков оцениваются Humanity's Last Exam (HLE) [34], AIME 2026, HMMT
2025, IMO-AnswerBench [29], GPQA-Diamond [39] и LongBench v2 [5]. Для
HLE оценивается только текстовое подмножество, а в качестве модели-судьи используется GPT-5.2 (medium).
Большинство задач рассуждений оцениваются с максимальной длиной генерации 131 072 токена, в то время как
для HLE-with-tools используется максимум 202 752 токена.

Из таблицы 7 видно, что GLM-5 достигает сравнимой производительности в задачах рассуждений с сильной открытой
базовой моделью Kimi-K2.5. По сравнению с проприетарными моделями, GLM-5 превосходит Claude Opus 4.5 и
Gemini 3 Pro на HLE (с инструментами). GLM-5 также достигает значительных улучшений на бенчмарке HLE
(как с инструментами, так и без них) по сравнению со своим предшественником GLM-4.7. На бенчмарках HMMT
февраль/ноябрь 2025 GLM-5 показывает лучшую производительность, чем Claude Opus 4.5 и Gemini 3
Pro. GLM-5 также демонстрирует значительный прогресс в задачах с длинным контекстом, что подтверждается достижением
наивысшего результата на бенчмарке рассуждений с длинным контекстом LongBench v2, уступая только Gemini 3 Pro.

22