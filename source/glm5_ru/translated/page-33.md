[21] J. Li, A. Fang, G. Smyrnis, M. Ivgi, and et al. Datacomp-lm: In search of the next generation of
training sets for language models, 2025.
[22] J. Li, W. Zhao, J. Zhao, W. Zeng, H. Wu, X. Wang, R. Ge, Y. Cao, Y. Huang, W. Liu, et al.
The tool decathlon: Benchmarking language agents for diverse, realistic, and long-horizon task
execution. arXiv preprint arXiv:2510.25726, 2025.
[23] R. Li, J. Fu, B.-W. Zhang, T. Huang, Z. Sun, C. Lyu, G. Liu, Z. Jin, and G. Li. Taco: Topics in
algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023.
[24] A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.
Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv
preprint arXiv:2405.04434, 2024.
[25] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.
[26] A. Liu, A. Mei, B. Lin, B. Xue, B. Wang, B. Xu, B. Wu, B. Zhang, C. Lin, C. Dong,
et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint
arXiv:2512.02556, 2025.
[27] J. Liu, J. Le Tian, V. Daita, Y. Wei, Y. Ding, Y. K. Wang, J. Yang, and L. ZHANG. Repoqa:
Evaluating long context code understanding. In First Workshop on Long-Context Foundation
Models@ ICML 2024.
[28] K. Lu and T. M. Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025.
https://thinkingmachines.ai/blog/on-policy-distillation.
[29] M.-T. Luong, D. Hwang, H. H. Nguyen, G. Ghiasi, Y. Chervonyi, I. Seo, J. Kim, G. Bingham,
J. Lee, S. Mishra, et al. Towards robust mathematical reasoning. In EMNLP'25, pages 35406â€“
35430, 2025.
[30] I. Moshkov, D. Hanley, I. Sorokin, S. Toshniwal, C. Henkel, B. Schifferer, W. Du, and I. Git-
man. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with
openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025.
[31] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A. Korthikanti, D. Vainbrand,
P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale
language model training on gpu clusters using megatron-lm, 2021.
[32] OpenAI. Introducing gpt 5.2, 2025.
[33] T. Patwardhan, R. Dias, E. Proehl, G. Kim, M. Wang, O. Watkins, S. P. Fishman, M. Aljubeh,
P. Thacker, L. Fauconnet, et al. Gdpval: Evaluating ai model performance on real-world
economically valuable tasks. arXiv preprint arXiv:2510.04374, 2025.
[34] L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, C. B. C. Zhang, M. Shaaban, J. Ling, S. Shi,
et al. Humanity's last exam. arXiv preprint arXiv:2501.14249, 2025.
[35] Prime Intellect. Synthetic-2 release: Four million collaboratively generated reasoning traces,
2025. Blog post.
[36] V. Pyatkin, S. Malik, V. Graf, H. Ivison, S. Huang, P. Dasigi, N. Lambert, and H. Hajishirzi.
Generalizing verifiable instruction following, 2025.
[37] P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism. arXiv preprint
arXiv:2401.10241, 2023.
[38] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training
trillion parameter models, 2020.
[39] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R.
Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In CoLM'24, 2024.
33