Оператор pop(·) подавляет образцы, чье отношение несоответствия отклоняется чрезмерно:
pop(ρi,t, 1/β, β) =
ρi,t,
1/β ≤ρi,t ≤β,
0,
иначе.
Коэффициент важности в стиле PPO и групповое нормализованное преимущество следуют исходному
определению GRPO:
ri,t = πtrain
θ
(yi,t | x, yi,<t)
πtrain
θold (yi,t | x, yi,<t),
ˆAi,t = Ri −mean(R1, . . . , RG)
std(R1, . . . , RG)
.
Во время обучения устанавливаются гиперпараметры β = 2, ϵlow = 0,2, ϵhigh = 0,28. Обучение выполняется
полностью on-policy с размером группы 32 и размером батча 32.
Выводы по RL с DSA.
Выполнено крупномасштабное обучение с подкреплением модели на основе
архитектуры DSA. По сравнению с MLA, DSA вводит дополнительный индексатор, который извлекает top-k
наиболее релевантных записей ключ-значение и вычисляет внимание разреженно по извлеченному подмножеству.
Результаты извлечения top-k критичны для стабильности RL. Это аналогично тому, как модели MoE используют
routing replay [62] для сохранения активированных top-k экспертов и обеспечения согласованности обучения и инференса. Однако
хранение индексов top-k индексатора в каждой позиции токена явно непрактично, поскольку k = 2 048,
используемое индексатором, значительно больше k, обычно используемого в MoE, и хранение всех этих индексов
влечёт огромные затраты памяти, а также значительные коммуникационные издержки между
движком обучения и движком инференса.
Обнаружено, что применение детерминированного оператора top-k эффективно решает эту проблему. По сравнению с
недетерминированной реализацией top-k на основе CUDA, используемой в DSA Indexer SGLang, прямое
использование наивного torch.topk немного медленнее, но детерминировано. Он производит более согласованные выходы
и обеспечивает существенные улучшения при RL. В отличие от этого, другие недетерминированные операторы top-k (например, реализации на CUDA или
TileLang) вызывали резкую деградацию производительности во время RL всего после нескольких шагов,
сопровождаемую резким падением энтропии. Поэтому на протяжении всех этапов RL используется torch.topk
в качестве оператора top-k по умолчанию в DSA Indexer в движке обучения. Параметры индексатора
также замораживаются по умолчанию во время RL для ускорения обучения и предотвращения нестабильного обучения в индексаторе.
RL с рассуждениями в смешанных доменах.
На этапе Reasoning RL выполняется смешанное обучение с подкреплением по
четырём доменам: математика, естественные науки, код и рассуждения с интеграцией инструментов (TIR). Для математики
и естественных наук данные курируются из открытых наборов данных [10; 30] и совместно разработанных коллекций
с внешними поставщиками аннотаций. Дополнительно применяется фильтрация по сложности для фокусировки обучения на задачах,
которые GLM-4.7 решает правильно только редко или стабильно не решает, оставаясь при этом решаемыми более сильными
моделями-учителями (например, GPT-5.2 xhigh и Gemini 3 Pro Preview). Для кода охватываются как задачи
в стиле соревновательного программирования, так и задачи научного программирования. Первые в основном берутся из
Codeforces и репрезентативных наборов данных, таких как TACO [23] и SYNTHETIC-2-RL [35], в то время как
вторые строятся из внутренних пулов задач путём декомпозиции вопросов на минимальные реализации кода,
требуемые для корректных решений. Для TIR повторно используется более сложное подмножество
данных RL по математике и естественным наукам, дополнительно совместно строятся вопросы по STEM с поставщиками аннотаций,
которые явно разработаны для ответа с внешними инструментами. Во время обучения с подкреплением назначаются специфичные для домена
и источника модели-судьи или системы оценки для получения бинарных наград за результат. Общая смесь
поддерживается примерно сбалансированной по четырём доменам, и стабильно наблюдаются устойчивые и
значительные улучшения в каждом домене при смешанной настройке RL.
3.3
Agentic RL
Для улучшения агентных возможностей GLM-5 разработан полностью асинхронный и разделённый фреймворк RL
и выполнена оптимизация GLM-5 в задачах агентов программирования и поиска. Наивный синхронный RL страдает
от серьёзного простоя GPU во время длительных роллаутов агента. Путём разделения движков инференса и обучения
через центральный Multi-Task Rollout Orchestrator достигается высокопроизводительное совместное обучение
на разнообразных агентных рабочих нагрузках.
Для поддержания стабильности обучения при асинхронных off-policy условиях введены два ключевых
механизма. Во-первых, шлюз Token-in-Token-out (TITO) устраняет несоответствия ретокенизации путём
сохранения точного соответствия на уровне действий. Во-вторых, применяется Direct Double-sided Importance
12