Файл: page-18

гарантировать, что реализация модели соответствует тестовому патчу. Мы используем конвейер настройки окружения на основе фреймворка RepoLaunch [59], который масштабирует построение исполняемых окружений из реальных задач SWE. Этот конвейер автоматически анализирует установку репозитория и настройку зависимостей для построения исполняемого окружения и генерации команд тестирования, затем использует LLM для генерации языково-зависимых функций парсинга логов из выходных данных тестов, что позволяет извлекать тестовые случаи Fail-to-Pass (F2P) и Pass-to-Pass (P2P). Используя этот конвейер, мы строим более 10 000 верифицируемых окружений в тысячах репозиториев, охватывающих 9 языков программирования, включая Python, Java, Go, C, CPP, JavaScript, TypeScript, PHP и Ruby.

4.2.2 Терминальные окружения

Синтез из исходных данных. Для масштабного построения верифицируемых окружений терминального агента мы разрабатываем агентный конвейер синтеза данных, состоящий из трех фаз: генерация черновика задачи, конкретная реализация задачи и итеративная оптимизация задачи. Начиная с набора исходных задач, собранных из реальных сценариев разработки программного обеспечения и работы с терминалом, мы использовали LLM для мозгового штурма и генерации большого пула черновиков верифицируемых терминальных задач. Затем эти черновики конкретизируются агентом построения в конкретные задачи в формате Harbor [42], включая структурированные описания задач, докеризованные окружения выполнения и соответствующие тестовые скрипты. Впоследствии агент уточнения проверяет и итеративно совершенствует сгенерированные задачи согласно вручную определённым критериям, гарантируя, что образы Docker могут быть надёжно построены, тестовые случаи согласованы со спецификациями задач, и окружения устойчивы к потенциальным эксплойтам или обходным путям. В целом конвейер создаёт тысячи разнообразных и верифицируемых окружений терминального агента с точностью построения Docker, превышающей 90%.

Синтез из веб-корпуса. Мы разрабатываем масштабируемый автоматизированный конвейер и строим верифицированные LLM терминальные задачи программирования на основе веб-корпуса, используя замкнутую схему, в которой агент построения также служит собственным оценщиком первого прохода. Сначала мы собираем крупномасштабный корпус веб-страниц, связанных с кодом, и применяем классификатор качества данных для сохранения только высококачественного контента, отбрасывая страницы, которые преимущественно нетехнические или не содержат существенного кодового контента. Из отфильтрованного подмножества мы дополнительно идентифицируем веб-страницы, подходящие для формулировки задач терминального типа. Затем мы применяем стратифицированную выборку по категориям тем и уровням сложности для обеспечения распределительного баланса и разнообразия в результирующем пуле задач. Во-вторых, мы передаём агенту кодирования спецификацию построения задач Harbor6, включая схему задачи, требования к форматированию и примеры задач, вместе с каждой выбранной исходной веб-страницей. Агенту предписывается (i) синтезировать полную терминальную задачу, основанную на содержимом веб-страницы, и (ii) выполнить скрипт валидации Harbor для собственного вывода. При неудаче валидации агент итеративно диагностирует и пересматривает задачу, пока она не пройдёт все автоматизированные проверки. Только задачи, успешно прошедшие этот цикл самопроверки, включаются в финальный набор данных.

4.2.3 Поисковые задачи

Для задач глубокого поиска информации мы строим конвейер синтеза данных, который производит сложные многошаговые пары вопрос-ответ. Каждый вопрос требует многоступенчатого рассуждения, основанного на доказательствах, агрегированных из множества веб-источников.

Построение графа веб-знаний (WKG) и генерация вопросов. Начиная с траекторий поискового агента ранней стадии, мы собираем и дедуплицируем все встреченные URL, сохраняя более двух миллионов высокоинформативных веб-страниц из различных доменов. LLM выполняет семантический парсинг для распознавания сущностей, фильтрации шума и структурированного извлечения информации. WKG непрерывно обновляется новыми страницами и совершенствуется с использованием сигналов верификации через выравнивание сущностей, нормализацию атрибутов, консолидацию отношений и коррекцию семантической согласованности. На основе WKG мы отбираем сущности с низкой и средней частотой в качестве начальных узлов и расширяем их многошаговые окрестности для формирования полных подграфов, контролируя расширение для уменьшения пересечений. Используя промпты, нацеленные на высокую сложность и многодоменное рассуждение, мы преобразуем каждый подграф в вопрос, неявно кодирующий цепочки многосущностных отношений.

Фильтрация и верификация высокосложных вопросов. Мы применяем трёхэтапный конвейер для балансировки сложности и корректности: (1) Удаляем вопросы, на которые модель рассуждения без инструментов правильно отвечает хотя бы в одной из восьми независимых попыток. (2) Фильтруем вопросы, решаемые агентом ранней стадии

6https://harborframework.com/docs/tasks/task-tutorial

18