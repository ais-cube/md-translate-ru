Таблица 6: Результаты бенчмарка RULER для GLM-4.7-Flash с DSA. Вариант warmup-only
обучает только индексатор при замороженной базовой модели, полный вариант DSA совместно обучает оба
компонента на 150B токенах.
4K
8K
16K
32K
64K
128K
GLM-4.7-Flash
97,44
96,72
95,83
92,96
85,34
79,21
GLM-4.7-Flash + DSA warmup
97,51
96,54
95,40
90,09
84,05
71,35
GLM-4.7-Flash + DSA
96,77
96,25
96,69
93,45
87,06
78,86
На этапе совместного обучения GLM-4.7-Flash + DSA практически полностью устраняет это остаточное отставание: модель превосходит базовую на 16K (+0,86), 32K (+0,49) и 64K (+1,72), демонстрируя лишь снижение на 0,35 пункта на 128K.

2.2
Данные предобучения

Web.
Развивая конвейер данных GLM-4.5, мы усовершенствовали критерии отбора для массивных веб-датасетов. Мы внедрили ещё один классификатор DCLM [21] на основе эмбеддингов предложений для выявления и агрегации дополнительных высококачественных данных помимо стандартных классификаторов. Для решения проблемы длиннохвостовых знаний мы использовали классификатор World Knowledge, оптимизированный на основе записей Wikipedia и данных, размеченных LLM, для извлечения ценной информации из данных среднего и низкого качества.

Code.
Мы расширяем корпус предобучения на коде свежими снимками с основных платформ хостинга кода и более крупной коллекцией веб-страниц, содержащих код, что приводит к увеличению на 28% уникальных токенов после нечёткой дедупликации. Для повышения целостности корпуса и снижения шума мы исправили проблемы выравнивания метаданных в файлах кода Software Heritage и приняли более точный конвейер классификации языков. Мы следуем стратегии сэмплирования с учётом качества GLM-4.5 для исходного кода и веб-документов, связанных с кодом. Кроме того, мы обучили специализированные классификаторы для более широкого набора низкоресурсных языков программирования (например, Scala, Swift, Lua и др.), улучшая качество сэмплирования для этих языков.

Math & Science.
Мы собираем высококачественные данные по математике и науке с веб-страниц, из книг и научных статей для дальнейшего повышения способностей к рассуждению. В частности, конвейеры извлечения контента для веб-страниц и механизмы парсинга PDF для книг и статей усовершенствованы для повышения качества данных. Мы используем большие языковые модели для оценки документов-кандидатов и сохраняем только наиболее образовательный контент. Для документов большого контекста мы разработали алгоритм оценки с разбиением на фрагменты и агрегацией для повышения точности оценки. Применяются конвейеры фильтрации для строгого исключения синтетических, AI-генерированных или шаблонных данных.

2.3
Промежуточное обучение

Развивая фреймворк промежуточного обучения, представленный в GLM-4.5, мы увеличили как объём обучения, так и максимальную длину контекста в GLM-5 для дальнейшего усиления способностей модели к рассуждению, обработке длинного контекста и агентным возможностям.

Расширенный контекст и масштаб обучения.
Мы поэтапно расширяем окно контекста в три этапа: 32K (1T токенов), 128K (500B токенов) и 200K (50B токенов). По сравнению с максимумом 128K в GLM-4.5, дополнительный этап 200K существенно улучшает способность модели обрабатывать сверхдлинные документы и сложные мультифайловые кодовые базы. Длинные документы и синтетические агентные траектории соответственно увеличиваются в выборке на поздних этапах.

Данные по разработке программного обеспечения.
Мы сохраняем парадигму конкатенации файлов кода на уровне репозитория, коммит-диффов, GitHub issues, pull requests и соответствующих исходных файлов в единые обучающие последовательности. В GLM-5 мы ослабили критерии фильтрации на уровне репозитория для расширения пула подходящих репозиториев, получив около 10 миллионов пар issue–PR, при этом усилив фильтрацию качества на уровне отдельных issue для снижения шума. Мы также извлекаем больший набор релевантных файлов для каждой пары issue–PR, что приводит к более богатым контекстам разработки и более широкому охвату реальных сценариев разработки программного обеспечения. После фильтрации часть датасета с issue–PR содержит приблизительно 160B уникальных токенов.

8