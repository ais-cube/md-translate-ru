Таблица 7: Сравнение GLM-5 с открытыми и проприетарными моделями. Результаты, отмеченные *,
получены на полном наборе HLE. Результаты, отмеченные †, оценены на проверенной версии Terminal-
Bench 2.0 с исправлением неоднозначных инструкций. Оценки Elo GDPval-AA зафиксированы 15 
февраля 2026 г. Наивысший результат для каждого теста выделен жирным шрифтом, второй по величине подчёркнут.
GLM-5 GLM-4.7 DeepSeek
-V3.2
Kimi
K2.5
Claude
Opus 4.5
Gemini
3 Pro
GPT-5.2
(xhigh)
Рассуждения и общие задачи
HLE
30,5
24,8
25,1
31,5
28,4
37,2
35,4
HLE (с инструментами)
50,4
42,8
40,8
51,8
43,4*
45,8*
45,5*
AIME 2026 I
92,7
92,9
92,7
92,5
93,3
90,6
-
HMMT Feb. 2025
97,9
97,1
92,5
95,4
92,9
97,3
99,4
HMMT Nov. 2025
96,9
93,5
90,2
91,1
91,7
93,0
97,1
IMO-AnswerBench
82,5
82,0
78,3
81,8
78,5
83,3
86,3
GPQA-Diamond
86,0
85,7
82,4
87,6
87,0
91,9
92,4
LongBench v2
64,5
59,1
59,8
61,0
64,4
68,2
59,8
Программирование
SWE-bench Verified
77,8
73,8
73,1
76,8
80,9
76,2
80,0
SWE-bench Multilingual
73,3
66,7
70,2
73,0
77,5
65,0
72,0
Terminal-Bench 2.0
(Terminus-2)
56,2 /
60,7†
41,0
39,3
50,8
59,3
54,2
54,0
Terminal-Bench 2.0
(Claude Code)
56,2 /
61,1†
32,8
46,4
-
57,9
-
-
CyberGym
43,2
23,5
17,3
41,3
50,6
39,9
-
Агентные задачи
BrowseComp
62,0
52,0
51,4
60,6
37,0
37,8
-
BrowseComp
(с управлением контекстом)
75,9
67,5
67,6
74,9
57,8
59,2
65,8
BrowseComp-ZH
72,7
66,6
65,0
62,3
62,4
66,8
76,1
τ 2-Bench
89,7
87,4
85,3
80,2
91,6
90,7
85,5
MCP-Atlas (Public Set)
67,8
52,0
62,2
63,8
65,2
66,6
68,0
Tool-Decathlon
39,2
23,8
35,2
27,8
43,5
36,4
46,3
Vending-Bench 2
$4 432
$2 377
$1 034
$1 198
$4 967
$5 478
$3 591
GDPval-AA Elo
1 409
1 198
1 195
1 288
1 400
1 201
1 462
6.1.2
Оценка на тестах по программированию
Для тестов по программированию мы оцениваем LLM на SWE-bench Verified [19], SWE-bench Multilin-
gual [53], Terminal Bench 2.0 [45] и CyberGym [48]. Для SWE-bench Verified и Multilingual
используется фреймворк OpenHands с адаптированным промптом инструкций для GLM-5. Для Terminal-
Bench 2.0 используются два агентных фреймворка (Terminus-2 и Claude Code), также приводятся
результаты на проверенной версии Terminal-Bench 2.0, которая устраняет некоторые неоднозначные инструкции8. 
Тест CyberGym оценивается в Claude Code 2.1.18.
Из таблицы 7 видно, что GLM-5 достигает передовых результатов на тестах по программированию среди открытых LLM.
В сравнении с проприетарными LLM, GLM-5 показывает лучшие результаты, чем Gemini 3 Pro на SWE-bench Verified,
а также превосходит Gemini 3 Pro и GPT-5.2 (xhigh) на SWE-bench Multilingual. На Terminal-Bench
2.0 GLM-5 достигает сопоставимых результатов с Claude Opus 4.5 и ещё лучших результатов при исправлении
неоднозначных инструкций в этом тесте. Для демонстрации обобщающей способности в программировании
мы проводим оценку на Terminal Bench 2.0 с двумя агентными фреймворками, и GLM-5 показывает стабильные
8Дополнительную информацию можно найти на https://huggingface.co/datasets/zai-org/terminal-bench
-2-verified
23