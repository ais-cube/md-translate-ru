Сэмплирование, которое применяет механизм обрезки на уровне токенов ([1−ϵℓ, 1+ϵh]) к логарифмам вероятностей роллаутов,
эффективно контролируя смещение off-policy без отслеживания исторических контрольных точек политик. Мы также
используем DP-aware маршрутизацию для максимизации повторного использования KV-кэша при длинноконтекстном выводе для крупномасштабных
MoE-моделей для ускорения. Для масштабирования агентных окружений мы масштабируем верифицируемые обучающие окружения
в трёх областях: более 10 000 реальных задач программной инженерии (SWE), терминальных задач и высоко-
сложных многошаговых поисковых задач. Больше деталей об агентном RL можно найти в последующем
разделе 4.
3.4
Общее RL
Многомерные цели оптимизации.
Мы разделяем цели оптимизации общего
RL на три взаимодополняющих измерения: базовая корректность, эмоциональный интеллект и
качество, специфичное для задачи.
Измерение базовой корректности служит основой качества ответов. Оно нацелено на
широкий спектр типов ошибок, подрывающих пригодность выходных данных модели, включая сбои в следовании инструкциям, логические несоответствия, фактические неточности, галлюцинации знаний и
языковую небеглость. Цель состоит в минимизации частоты ошибок, чтобы ответы достигали базового уровня пригодности.
Мы считаем это предварительным условием для всей последующей оптимизации: ответ, содержащий фактические ошибки или
неверно интерпретирующий намерения пользователя, может активно вводить пользователя в заблуждение, независимо от того, насколько отполированным он может выглядеть.
Измерение эмоционального интеллекта оптимизирует пользовательский опыт за пределами основной корректности. Оно направлено на
создание ответов, которые являются эмпатичными, проницательными и стилистически близкими к естественной человеческой коммуникации, делая взаимодействие с моделью более естественным и увлекательным.
Измерение качества, специфичного для задачи, нацелено на детальную оптимизацию в различных конкретных задачах.
Опираясь на пригодность, установленную базовой корректностью, оно направлено на повышение качества ответов от
просто правильных до действительно высококачественных в каждой категории задач. Это измерение охватывает широкий
спектр задач, включая написание текстов, обработку текста, субъективные и объективные ответы на вопросы, ролевые игры и перевод. Каждая область задач требует отдельных сигналов вознаграждения, что обуславливает необходимость гибридной
системы вознаграждения.
Гибридная система вознаграждения.
Для контроля вышеупомянутых различных целей мы создаём гибридную систему вознаграждения,
которая интегрирует три взаимодополняющих типа сигналов вознаграждения: функции вознаграждения, основанные на правилах, модели вознаграждения на основе результата (ORM) и генеративные модели вознаграждения (GRM). Каждый тип имеет отдельные сильные и
слабые стороны, и их комбинация является ключом к стабильному, эффективному и масштабируемому процессу обучения общего RL.
Вознаграждения на основе правил обеспечивают точные и интерпретируемые сигналы, но ограничены аспектами, выражаемыми
детерминированными правилами. ORM предлагают сигналы с низкой дисперсией и высокой эффективностью обучения, но более
подвержены взлому вознаграждения, когда политика эксплуатирует поверхностные паттерны, а не действительно улучшает базовую способность. GRM используют языковые модели для создания скалярных или структурированных оценок
и более устойчивы к такой эксплуатации, но склонны демонстрировать более высокую дисперсию. Комбинируя эти три
типа сигналов, мы получаем систему вознаграждения, которая балансирует точность, эффективность и устойчивость, смягчая
слабые стороны любого отдельного компонента.
Выравнивание в стиле human-in-the-loop.
Отличительным аспектом нашего конвейера общего RL является явное
включение высококачественных ответов, написанных людьми. Вместо того чтобы полагаться исключительно на модельно-
генерированные ответы, мы вводим экспертные человеческие ответы в качестве стилистических и качественных якорей. Это
мотивировано наблюдением, что чисто модельно-генерируемая оптимизация склонна сходиться к
узнаваемо "модельным" паттернам—часто многословным, шаблонным или лишённым нюансов умелого человеческого
письма. Представляя модель написанным людьми образцам, мы побуждаем её принимать более естественные,
человеко-ориентированные паттерны ответов.
3.5
On-Policy межстадийная дистилляция
В нашем многостадийном конвейере RL последовательная оптимизация для различных целей может привести к накопительной деградации ранее приобретённых способностей. Для смягчения этой проблемы мы выполняем on-policy
межстадийную дистилляцию в качестве финального этапа, применяя алгоритм on-policy дистилляции [14; 52; 51; 28]
для быстрого восстановления навыков, приобретённых на более ранних стадиях SFT и RL (Reasoning RL и General RL).
В частности, финальные контрольные точки из предыдущих стадий обучения служат моделями-учителями, где
13