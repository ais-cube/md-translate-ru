Таблица 11: Сравнение GLM-5-Base, GLM-4.5-Base и других репрезентативных open-source базовых моделей.

| Бенчмарк (метрика) | DeepSeek-V3<br>Base | Kimi-K2<br>Base | GLM-4.5<br>Base | GLM-5<br>Base |
|---|---|---|---|---|
| Архитектура | MoE | MoE | MoE | MoE |
| # Активированных параметров | 37B | 32B | 32B | 40B |
| # Всего параметров | 671B | 1043B | 355B | 744B |
| Английский | | | | |
| SimpleQA (EM) | 26,6 | 35,3 | 30,0 | 36,0 |
| BBH (EM) | 88,4 | 88,7 | 86,2 | 87,4 |
| MMLU (EM) | 87,2 | 87,8 | 86,1 | 88,3 |
| HellaSwag (EM) | 88,9 | 94,6 | 87,1 | 88,1 |
| PIQA (EM) | 84,7 | - | 85,3 | 84,6 |
| TriviaQA (EM) | 82,9 | 85,1 | 80,0 | 80,9 |
| Код | | | | |
| EvalPlus (Pass@1) | 65,6 | 80,3 | 78,1 | 87,0 |
| LiveCodeBench-Base (Pass@1) | 24,6 | 26,3 | 28,1 | 34,4 |
| Математика | | | | |
| GSM8K (EM) | 87,6 | 92,1 | 79,4 | 68,8 |
| MATH (EM) | 62,6 | 70,2 | 61,0 | 56,4 |
| Китайский | | | | |
| CLUEWSC (EM) | 82,7 | - | 83,5 | 84,2 |
| C-Eval (EM) | 90,1 | 92,5 | 86,9 | 88,8 |
| C3 (EM) | 78,6 | - | 83,1 | 80,3 |
| Chinese-SimpleQA (EM) | 72,1 | 77,6 | 70,1 | 74,6 |

Terminal-Bench 2.0 (Terminus 2): Мы проводим оценку с использованием фреймворка Terminus с timeout = 2h, temperature = 0,7, top_p = 1,0, max_new_tokens = 8192, с окном контекста 128K. Ограничения ресурсов установлены на уровне 16 CPU и 32 ГБ RAM.

Terminal-Bench 2.0 (Claude Code): Мы проводим оценку в Claude Code 2.1.14 (режим think) с temperature = 1,0, top_p = 0,95, max_new_tokens = 65536. Мы снимаем ограничения по реальному времени, сохраняя при этом ограничения CPU и памяти для каждой задачи. Мы исправляем проблемы окружения, вызванные Claude Code, и также приводим результаты на верифицированном наборе данных Terminal-Bench 2.0, который устраняет неоднозначные инструкции (см.: https://huggingface.co/datasets/zai-org/terminal-bench-2-verified). Оценки усреднены по 5 запускам.

CyberGym: Мы проводим оценку в Claude Code 2.1.18 (режим think, без веб-инструментов) с (temperature = 1,0, top_p = 1,0, max_new_tokens = 32000) и таймаутом 250 минут на задачу. Результаты представляют собой Pass@1 в одном запуске по 1 507 задачам.

MCP-Atlas: Все модели оцениваются в режиме think на публичном подмножестве из 500 задач с таймаутом 10 минут на задачу. В качестве модели-судьи мы используем Gemini 3 Pro.

τ 2-Bench: Мы добавляем небольшую корректировку промпта в Retail и Telecom, чтобы избежать сбоев, вызванных преждевременным завершением работы пользователем. Для Airline мы применяем исправления для домена, предложенные в системной карте Claude Opus 4.5.

Vending-Bench 2: Запуски проводятся независимо компанией Andon Labs10.

B.3 Оптимизированный симулятор пользователя для τ 2-Bench

Мы добавляем небольшую корректировку промпта в Telecom и Retail, чтобы избежать сбоев, вызванных преждевременным завершением работы пользователем. Оптимизированные промпты показаны на рисунке 12 и рисунке 13. Эти оптимизированные промпты интегрируются в системный промпт следующим образом:

```
SYSTEM_PROMPT = """"
{global_user_sim_guidelines}
```

10https://andonlabs.com/evals/vending-bench-2

37