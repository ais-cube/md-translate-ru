Формально, целевую функцию оптимизации с отсечением на уровне токенов можно записать как:
L(θ) = Et
h
f(rt(θ), ϵl, ϵh) ˆAt log πθ(at|st)
i
(3)
В данной формулировке коэффициент importance sampling rt(θ) вычисляется как:
rt(θ) = exp (log πθ(at|st) −log πrollout(at|st))
(4)
Стабильность дополнительно обеспечивается с помощью калибровочной функции f(x; ϵℓ, ϵh):
f(x; ϵℓ, ϵh) =
x,
если 1 −ϵℓ< x < 1 + ϵh
0,
в противном случае
(5)
В экспериментах мы обнаруживаем, что повторное использование логарифмических вероятностей rollout допускает контролируемую степень off-policy смещения, что позволяет обойти необходимость отслеживания исторических политик при одновременном повышении стабильности обучения.
Отбрасывание off-policy и зашумлённых образцов.
В асинхронном RL чрезмерно длинные траектории могут сильно отклоняться от текущей политики (off-policy), что может дестабилизировать обучение. Для фильтрации таких сильно отклонённых образцов мы логируем версию весов политики, использованную движком rollout во время генерации. Конкретно, для каждого ответа мы записываем последовательность задействованных версий модели (w0, . . . , wk) с w0 < · · · < wk.
Пусть w′ обозначает текущую версию политики. Мы отбрасываем образец, если его самая старая версия rollout слишком устарела, т.е. если w′ −w0 > τ, где τ — заранее определённый порог. Это удаляет траектории, которые слишком сильно отстают от текущей политики.
Кроме того, песочницы coding-agent могут быть по своей природе нестабильными и давать сбои по причинам, не связанным с моделью (например, сбои окружения). Такие сбои создают зашумлённые обучающие сигналы, поскольку они отражают нестабильность окружения, а не возможности модели. Для снижения этого эффекта мы записываем причину сбоя для каждого образца и исключаем образцы, которые дают сбой из-за краха окружения. Для методов группового семплирования, таких как GRPO, удаление неудачных образцов может оставить неполную группу. В этом случае мы дополняем группу повторением валидных образцов, если количество валидных образцов превышает половину размера группы; в противном случае мы отбрасываем всю группу. Эта процедура снижает ложный шум вознаграждения и улучшает стабильность обучения.
DP-aware маршрутизация для ускорения.
Мы предлагаем механизм DP-aware маршрутизации для сохранения локальности KV-кеша при параллелизме по данным (DP) для крупномасштабного MoE-инференса. В многошаговых агентных рабочих нагрузках последовательные запросы из одного и того же rollout имеют идентичный префикс. Для максимизации повторного использования KV мы обеспечиваем привязку на уровне rollout: все запросы, принадлежащие данному экземпляру агента, маршрутизируются на один и тот же DP-ранг. Конкретно, мы вводим stateful-слой маршрутизации, который сопоставляет каждый rollout ID с фиксированным DP-рангом с помощью консистентного хеширования. Это сопоставление остаётся стабильным между шагами, исключая промахи кеша между рангами. Для предотвращения долгосрочного дисбаланса мы комбинируем хеширование с лёгкой динамической перебалансировкой нагрузки в пространстве хешей. Такая конструкция исключает избыточные вычисления prefill без необходимости синхронизации KV между DP-рангами. По мере увеличения длины rollout стоимость prefill остаётся пропорциональной инкрементным токенам, а не общей длине контекста. Результатом является улучшенная сквозная задержка и более высокая эффективная пропускная способность для длинноконтекстного агентного инференса.
4.2
Масштабирование окружений для агентов
Для поддержки обучения с подкреплением в различных агентных задачах мы конструируем верифицируемые исполняемые окружения, которые обеспечивают обоснованную обратную связь как для кодоцентричных рабочих процессов, так и для генерации контента.
Для агентных задач программирования мы разрабатываем два конвейера построения окружений, которые создают верифицируемые исполняемые окружения: конвейер настройки окружений, построенный на реальных задачах разработки программного обеспечения, и синтетический конвейер для окружений terminal-agent. Помимо программирования, мы также представляем окружение генерации слайдов, в котором агент работает со структурированным HTML с исполняемым рендерингом и верификацией на основе компоновки.
4.2.1
Окружения разработки программного обеспечения (SWE)
Перед конструированием исполняемых окружений мы собираем большой корпус реальных пар Issue-Pull Request (PR) и применяем строгую фильтрацию на основе правил и LLM для обеспечения получения аутентичных высококачественных формулировок задач. Мы категоризируем эти экземпляры по различным типам задач — исправление ошибок, реализация функций, рефакторинг и другие — и включаем необходимые требования к задачам для
17