0
200
400
600
800
1000
1200
1400
1600
1800
Шаги
50
55
60
65
70
75
80
85
BrowseComp
Модель / Стратегия
GLM-5 Pass@K
GLM-5 Fewest-step
GLM-5 HCM
GLM-4.7 Fewest-step
GLM-4.7 Discard-all
Рисунок 8: Точность BrowseComp с различными стратегиями управления контекстом от GLM-4.7
(серые базовые линии) до GLM-5 (цветные стратегии).
с базовым поиском, просмотром и вычислениями в несколько шагов. (3) Применить агента проверки для
двунаправленной валидации: мы собираем кандидатов-ответов из траекторий поиска на этапе 2, затем
независимо проверяем согласованность вопроса и ответа как для кандидатов, так и для аннотированной
эталонной истины, отклоняя образцы с неуникальными ответами, противоречивыми доказательствами или неправильными метками.
Это дает высококачественные, сложные, надежные пары многошаговых вопросов и ответов.
4.2.4
Вывод с управлением контекстом для агентов поиска
Мы обнаружили, что производительность на BrowseComp [50] чувствительна как к промпту судьи, так и к
модели судьи, и судьи с открытым исходным кодом могут вносить систематическое смещение. Чтобы обеспечить согласованность и
воспроизводимость, мы стандартизируем все компоненты, основанные на судье, используя официальный промпт оценки OpenAI
и проприетарную модель o3-mini в качестве судьи. Тематические исследования показывают, что эта конфигурация лучше всего соответствует
эталонной истине, размеченной человеком, поэтому мы принимаем её для всех экспериментов.
Предыдущие работы [26] ввели управление контекстом, где Discard-all сбрасывает контекст, уда-
ляя всю историю вызовов инструментов. Мы дополнительно наблюдаем, что точность модели существенно снижается
при чрезвычайно длинных контекстах (например, свыше 100 000 токенов).
На основе этого мы
применяем простую стратегию Keep-recent-k. Когда история взаимодействия превышает порог k,
содержимое старше последних k раундов сворачивается для контроля длины контекста. Пусть
траектория будет (q, r1, a1, o1, r2, a2, o2, · · · , rn, an, on), где q обозначает вопрос, ri обо-
значает рассуждение на раунде i, ai действие (мы проектируем 4 инструмента: search, open, find и python),
и oi наблюдение инструмента. Мы сворачиваем только наблюдения раньше последних k раундов:
oi ←Tool result is omitted to save tokens.
i = 1, . . . , n −k. В наших экспериментах мы устанавливаем k = 5,
что дает стабильное улучшение и повышает GLM-5 с 55,3% (без keep-recent-k) до
62,0% (с keep-recent-k). Мы также обнаружили, что использование различных значений keep recent k или аль-
тернативное срабатывание keep-recent при достижении длиной контекста заданного порога токенов приводит к
одинаковым результатам.
На основе этого мы объединяем keep-recent с Discard-all для формирования гибридной стратегии иерархического управления
контекстом. Во время вывода с keep-recent, если общая длина контекста превышает
порог T, мы отбрасываем всю историю вызовов инструментов и перезапускаем со свежим контекстом, продолжая
применять стратегию keep-recent. Мы выбираем T = 32 000 через параметрический поиск.
Как показано на рисунке 8, при различных вычислительных бюджетах эта стратегия эффективно освобождает пространство контекста,
позволяя модели выполнять больше шагов и последовательно улучшая производительность. По сравнению с использованием
только Discard-all, комбинация с keep-recent-k достигает стабильного прироста во всех бюджетах, достигая
финального результата 75,9, превосходя все модели с открытым исходным кодом, оснащенные управлением контекстом.
19