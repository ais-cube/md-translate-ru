паттерн демонстрирует надёжную обобщаемость по длине, сохраняя эффективность на всех протестированных длинах контекста.
• SimpleGDN: минималистичная стратегия линеаризации, разработанная для максимального повторного использования предобученных весов, улучшающая GDN для адаптации путём непрерывного обучения. Мы полностью удаляем модули Conv1d и явного gating и вместо этого напрямую отображаем предобученные веса проекций Query, Key и Value в формулировку линейной рекуррентности. Это упрощение устраняет необходимость в дополнительных параметрах, сохраняя при этом преимущества по эффективности линейного внимания.
Таблица 4: Результаты бенчмарка RULER для базовой модели GLM-9B и двух вариантов SWA без какого-либо дополнительного обучения. Оба метода SWA используют соотношение 1:1 слоёв полного внимания к слоям SWA с размером окна 4096 токенов. Паттерн SWA на основе поиска определяется один раз при длине контекста 16k и применяется единообразно для всех длин входных данных.
4K
8K
16K
32K
64K
128K
GLM-9B (Full Attn)
95,19
93,67
92,01
91,09
85,35
75,28
SWA Interleave
94,87
54,02
25,89
12,61
8,32
6,51
SWA Pattern
95,78
92,54
88,92
82,52
70,23
53,95
Мы оцениваем все методы на четырёх бенчмарках для длинного контекста: RULER [17], MRCR2, HELMET-ICL [56] и RepoQA [27]. Результаты приведены в таблице 5. Мы выполняем непрерывное обучение каждого метода на 190 млрд токенов с длиной контекста 64K, сохраняя соотношение 1:1 между слоями эффективного внимания и слоями полного внимания. Для методов GDN и SimpleGDN мы следуем конвейеру Jet-Nemotron [15].
Таблица 5: Результаты бенчмарков для длинного контекста. Все варианты эффективного внимания дообучены на основе базовой модели GLM-9B с полным вниманием. SWA pattern обозначает выбор слоёв на основе поиска; SWA interleave обозначает фиксированный чередующийся паттерн. ∆@64K и ∆@128K показывают разницу относительно базовой модели с полным вниманием при длине контекста 64K и 128K соответственно.
RULER (64K / 128K)
MRCR (64K / 128K)
HELMET-ICL (64K / 128K)
RepoQA (64K / 128K)
GLM-9B
85,35 / 75,28
36,53 / 35,39
77,68 / 77,36
69,00 / 65,83
SWA Interleave
65,94 / 44,93 (↓19,41 / ↓30,35)
30,03 / 28,83 (↓6,50 / ↓6,56)
75,96 / 63,52 (↓1,72 / ↓13,84)
50,33 / 39,33 (↓18,67 / ↓26,50)
SWA Pattern
83,72 / 69,59 (↓1,63 / ↓5,69)
35,02 / 33,58 (↓1,51 / ↓1,81)
76,48 / 74,60 (↓1,20 / ↓2,76)
62,33 / 51,17 (↓6,67 / ↓14,66)
GDN
76,76 / 64,00 (↓8,59 / ↓11,28)
31,72 / 30,22 (↓4,81 / ↓5,17)
76,88 / 74,84 (↓0,80 / ↓2,52)
65,50 / 56,17 (↓3,50 / ↓9,66)
SimpleGDN
81,76 / 67,03 (↓3,59 / ↓8,25)
33,03 / 31,27 (↓3,50 / ↓4,12)
79,80 / 81,84 (↑2,12 / ↑4,48)
65,50 / 58,50 (↓3,50 / ↓7,33)
Результаты в таблице 5 выявляют чёткую иерархию компромиссов между методами эффективного внимания. Наивно чередующееся скользящее оконное внимание (SWA) вызывает катастрофическую деградацию на задачах с длинным контекстом (например, −30,35 на RULER@128K), в то время как выбор слоёв на основе поиска существенно сокращает этот разрыв за счёт сохранения полного внимания там, где это наиболее важно. Варианты линейного внимания, такие как GDN, дополнительно улучшают качество, но ценой дополнительных параметров; SimpleGDN обеспечивает лучший баланс за счёт максимального повторного использования предобученных весов. Тем не менее, все эти методы несут присущий им разрыв в точности на задачах детального поиска — до 5,69 балла на RULER@128K и 7,33 на RepoQA@128K — вследствие неизбежной потери информации, вносимой механизмами эффективного внимания при адаптации путём непрерывного обучения, даже когда половина слоёв сохраняет полное внимание. В отличие от них, DSA является безубыточным по конструкции: его lightning-индексатор достигает разреженности на уровне токенов без отбрасывания каких-либо дальних зависимостей, позволяя применять его ко всем слоям без ухудшения качества.
Для проверки этого мы провели небольшой эксперимент с DSA на GLM-4.7-Flash3 с multi-latent вниманием. Следуя стандартному рецепту DSA, обучение проходит в два этапа: (i) фаза прогрева, которая обучает только индексатор в течение 1 000 шагов (размер батча 16) при заморожённых весах базовой модели, за которой следует (ii) фаза совместного обучения, в которой и модель, и индексатор одновременно обучаются на 150 млрд токенов. Таблица 6 суммирует результаты на RULER для длин контекста от 4K до 128K. Даже вариант только с прогревом (GLM-4.7-Flash + DSA warmup) уже сохраняет подавляющее большинство базовой производительности; падение умеренное и сконцентрировано на самом длинном контекстном окне (128K: 79,21 →71,35), в то время как более короткие контексты остаются практически не затронутыми. После полного обучения на 150 млрд токенов
2https://huggingface.co/datasets/openai/mrcr
3https://huggingface.co/zai-org/GLM-4.7-Flash
7