производительности в обеих средах. На тесте по программированию в области кибербезопасности (т.е. CyberGym) GLM-5 демонстрирует значительное улучшение по сравнению с GLM-4.7, уступая только Claude Opus 4.5.

6.1.3 Оценка агентных способностей

Для агентных тестов мы оцениваем GLM-5 и передовые модели на BrowseComp [50], BrowseComp-ZH [63], τ 2-Bench [7], MCP-Atlas [6], Tool-Decathlon [22], Vending-Bench 2 [3] и GDPval-AA [33]. BrowseComp измеряет, как языковые агенты решают сложные задачи путём просмотра веб-страниц, а BrowseComp-ZH ориентирован в основном на китайский интернет. Мы используем стратегию полного отбрасывания в качестве управления контекстом для BrowseComp, что соответствует подходу DeepSeek-V3.2 и Kimi K2.5. τ 2-Bench оценивает способность диалоговых агентов в среде с двойным управлением. Мы добавили небольшую корректировку промпта для Retail и Telecom, чтобы избежать сбоев, вызванных преждевременным завершением работы пользователя (см. B.3). Для Airline мы применяем доменные исправления, предложенные в системной карте Claude Opus 4.5 [1], чтобы получить более точные результаты. MCP-Atlas — это реальный тест использования инструментов, который оценивает производительность LLM в многошаговых рабочих процессах при наличии серверов Model Context Protocol (MCP). Для справедливого сравнения мы переоценили все модели на публичном наборе из 500 задач и увеличили таймаут с 4 до 10 минут на задачу, чтобы избежать сбоев задач из-за условий развёртывания. Мы используем Gemini 3 Pro в качестве модели-судьи для MCP-Atlas. Tool-Decathlon также является тестом использования инструментов, но ориентирован на реальные задачи с большим горизонтом планирования. Vending-Bench 2 измеряет агентные способности LLM в бизнес-сценарии с длительным горизонтом в симулированной среде, что добавляет больше реальных факторов к предшественнику Vending-Bench. GDPval фокусируется на том, как агенты ИИ справляются с экономически ценными задачами.

Из таблицы 7 видно, что GLM-5 значительно улучшается на агентных тестах по сравнению с GLM-4.7. На BrowseComp GLM-5 достигает SOTA производительности среди передовых LLM как с управлением контекстом, так и без него. На BrowseComp-ZH GLM-5 также превосходит Claude Opus 4.5 и Gemini 3 Pro. Для трёх агентных задач использования инструментов (т.е. τ 2-Bench, MCP-Atlas и Tool-Decathlon) GLM-5 достигает сопоставимой производительности с Claude Opus 4.5, что демонстрирует сильные способности GLM-5 в использовании инструментов. Производительность GLM-5 на Vending-Bench 2 (т.е. $4 432) дополнительно демонстрирует способность модели работать с долгосрочными бизнес-задачами. В экономических сценариях GLM-5 показывает лучшие результаты, чем Claude Opus 4.5, на GDPval-AA, уступая только GPT-5.2 (xhigh).

6.2 Оценка реального агентного инженерного опыта

Реальный опыт имеет большее значение, чем таблицы лидеров. Мы обновили наш внутренний CC-Bench до CC-Bench-V2 для оценки того, может ли модель правильно выполнять сквозные задачи в реалистичных агентных инженерных средах, охватывающих frontend, backend и задачи с большим горизонтом планирования. CC-Bench-V2 полностью исключает ручную разметку и полностью автоматизирован через Claude Code и другие агентные платформы с модульными тестами и техниками Agent-as-a-Judge.

Frontend. Мы используем конвейер для сначала сборки frontend-проектов, созданных агентом, и проверки на наличие синтаксических ошибок, ошибок зависимостей и совместимости. Затем мы используем Agent-as-a-Judge для валидации сквозной корректности путём имитации взаимодействия пользователя через GUI-агента, оснащённого инструментами Playwright и bash.

Backend. Задачи взяты из реальных open-source проектов на C++, Rust, Go, Java, TypeScript и Python, охватывающих реализацию функций, исправление ошибок, восстановление регрессий и оптимизацию производительности. Каждое изменение должно пройти полные модульные тесты в рамках реалистичных инженерных ограничений.

Долгосрочные задачи. Сначала мы оцениваем способность модели искать информацию в больших кодовых базах — необходимое условие для определения правильных файлов и понимания контекста проекта, как это делал бы разработчик-человек. Затем мы оцениваем сквозную корректность через многошаговые цепочки задач, построенные путём анализа объединённых Pull Request с обширной историей коммитов и кластеризации их коммитов в связные цепочки задач. Агент выполняет эти цепочки последовательно, проверяя свою способность сохранять контекст и разрешать зависимости между этапами. Оценка сочетает модульные тесты с Agent-as-a-Judge для проверки как функциональной корректности, так и семантического соответствия.

6.2.1 Оценка Frontend — Agent-as-a-Judge

Мы разработали комплексный автоматизированный эталонный тест, специально разработанный для сценариев frontend-разработки. Этот тест охватывает разнообразный спектр приложений, которые разработчики

24