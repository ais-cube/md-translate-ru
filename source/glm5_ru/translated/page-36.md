A
Гиперпараметры
Гиперпараметры, связанные с архитектурой модели GLM-5, показаны в таблице 10.
Для обучения мы следуем настройкам GLM-4.5, включая оптимизатор Muon, косинусное затухание и
warmup размера батча. Скорость обучения проходит через стадию warmup от 0 до 2e-4 и стадию затухания
до 4e-5 до конца этапа предобучения. На этапе промежуточного обучения скорость обучения
уменьшается линейно от 4e-5 до 1e-5. Другие гиперпараметры те же, что и у GLM-4.5. Для
стадии warmup DSA скорость обучения снижается от 5e-3 до 2e-4. Для стадии разреженной адаптации DSA
мы используем постоянную скорость обучения 1e-5.
Таблица 10: Архитектура модели GLM-4.5 и GLM-5. При подсчёте параметров для всех моделей мы
включаем параметры слоёв MTP, но не включаем словарные эмбеддинги и выходной слой.
| Модель | GLM-4.5 | GLM-5 |
|---|---|---|
| Всего параметров | 355B | 744B |
| Активированных параметров | 32B | 40B |
| Плотных слоёв | 3 | 3 |
| Слоёв MoE | 89 | 75 |
| Слоёв MTP | 1 | 1 |
| Размерность скрытого слоя | 5120 | 6144 |
| Промежуточная размерность плотного слоя | 12 288 | 12 288 |
| Промежуточная размерность MoE | 1536 | 2048 |
| Размерность головы QK | 128 | 192 |
| Размерность головы V | 128 | 256 |
| Размерность LoRA Q | – | 2048 |
| Размерность LoRA KV | – | 512 |
| Голов внимания | 96 | 64 |
| Голов Key-Value | 8 | – |
| Голов внимания индексера | – | 32 |
| Размерность головы индексера | – | 128 |
| Экспертов (всего) | 160 | 256 |
| Маршрутизируемых экспертов | 8 | 8 |
| Общих экспертов | 1 | 1 |
| Размер словаря | 151 552 | 154 880 |

B
Детали оценки
B.1
Оценка базовых моделей
Мы оцениваем базовую модель GLM-5 с помощью английских, китайских бенчмарков по коду и математике в таблице 11.
B.2
Оценка бенчмарков ARC
Humanity's Last Exam (HLE) и другие задачи на рассуждения: Мы проводим оценку с максимальной длиной генерации 131 072 токена (temperature = 1.0, top_p = 0.95, max_new_tokens = 131072). По
умолчанию мы сообщаем результаты для подмножества только с текстом; результаты, отмеченные *, получены для полного набора. Мы используем GPT-5.2
(medium) в качестве модели-судьи. Для HLE-with-tools мы используем максимальную длину контекста 202 752
токена.
SWE-bench и SWE-bench Multilingual: Мы запускаем набор SWE-bench с OpenHands, используя
специализированный промпт инструкций. Настройки: temperature = 0.7, top_p = 0.95, max_new_tokens =
16384, с окном контекста 200K.
BrowseComp: Без управления контекстом мы сохраняем детали из последних 5 ходов. С
управлением контекстом мы используем ту же стратегию discard-all, что и DeepSeek-V3.2 и Kimi K2.5.
36