MENT-SNS.
Для дальнейшей оценки устойчивости в лингвистически сложных контекстах мы используем исходные предложения из MENT [46], состоящие из 753 англо-китайских пар предложений по четырём доменам: социальные сети (SNS), межкультурная коммуникация, поэзия и литература. Эти домены выбраны для стресс-тестирования перевода в условиях сложных лингвистических явлений, включая сленг, омофоническую игру слов, идиоматические выражения, исторические отсылки и метафорический язык. Аналогично ZMultiTrans-Bench, все образцы были отобраны и проверены профессионально подготовленными аспирантами. Оценка следует тому же протоколу попарного сравнения с базовым ответом, при этом GPT-4.1 выступает в качестве автоматизированной модели-судьи.

6.3.2 Многоязычный диалог

LMArena.
Мы приводим рейтинги Elo из LMArena9, которые получены на основе масштабных попарных сравнений, предоставленных сообществом. Эти рейтинги отражают относительное предпочтение моделей в условиях открытого диалога и предоставляют внешний сигнал производительности в разговорном режиме.

ZMultiDialBench.
В дополнение к публичной таблице лидеров мы также проводим человеческую оценку на ZMultiDialBench, внутреннем многоязычном эталонном тесте для диалогов. Набор данных состоит из 141 отобранного экземпляра, охватывающего различные категории диалогов. Образцы были собраны из высококачественных разговорных данных, предоставленных носителями языка-аннотаторами из нескольких стран, а также из сложных случаев сбоев, о которых сообщали пользователи онлайн. Аннотаторы-люди присваивали поточечные оценки по шкале от 1 до 10 анонимизированным ответам моделей в соответствии со стандартизированными критериями оценки для конкретной категории.

6.3.3 Следование инструкциям

IF-Badcase.
IF-Badcase — это внутренний эталонный тест, построенный на случаях сбоев в следовании инструкциям, о которых сообщали реальные пользователи в производственных условиях. Набор данных предназначен для оценки строгого соблюдения реалистичных инструкций с множественными ограничениями, с акцентом на процедурную точность, логическую согласованность и жёсткие требования к форматированию. Оценка проводится с использованием подробного протокола на основе контрольного списка, который проверяет соответствие явным ограничениям, включая упорядоченные шаги, условия на основе правил и структурные спецификации. Все образцы были аннотированы, проверены и итеративно отфильтрованы экспертами-людьми, что привело к отобранному набору из 450 тестовых экземпляров.

IF-Bench [36].
IF-Bench оценивает LLM на их способность следовать сложным, объективным ограничениям, таким как специфические правила форматирования, ограничения по длине и ограничения по содержанию. Он предоставляет количественную меру возможностей точного следования инструкциям, фокусируясь на проверяемом соответствии, а не на качестве открытой генерации.

MultiChallenge [41].
MultiChallenge исследует LLM через реалистичные многоходовые разговорные сценарии. Он нацелен на сложные взаимодействия, требующие точного следования инструкциям, распределения контекста и рассуждений в контексте.

6.3.4 Знания о мире

SimpleQA [49].
SimpleQA измеряет фактическую точность в краткой форме, используя сложные вопросы с единственными, неоспоримыми ответами. Он оценивает калибровку модели, классифицируя ответы как правильные, неправильные или без попытки, отдавая приоритет точности над длиной генерации.

Chinese SimpleQA [16].
Адаптируя методологию SimpleQA к китайскому контексту, этот эталонный тест оценивает фактическую точность по шести основным доменам и 99 подтемам. Он использует высококачественные статические вопросы с краткими ответами, предназначенные для надёжной автоматизированной оценки точности знаний LLM.

6.3.5 Вызов инструментов

ToolCall-Badcase.
ToolCall-Badcase — это внутренний эталонный тест, полученный из случаев сбоев в сценариях вызова инструментов, о которых сообщали пользователи в производственных средах. Каждый экземпляр связан с проверяемым эталонным вызовом инструмента, что позволяет объективно оценить как выбор инструмента, так и аргументы

9https://arena.ai/leaderboard/text

29