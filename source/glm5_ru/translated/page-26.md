в Docker-контейнере и собирается для проверки статической корректности. Успешно собранные экземпляры затем передаются
автономному агенту-судье (Claude Code с Claude Sonnet 4.5, оснащённому инструментом Playwright
MCP), который работает в циклах с замкнутым контуром: для каждого проверяемого элемента агент читает исходный код,
взаимодействует с активным пользовательским интерфейсом (клики, нажатия клавиш, скриншоты), проверяет вывод терминала и выносит
вердикт о прохождении или провале.
Для проверки надёжности мы сравниваем вердикты агента-судьи с независимыми экспертными
оценками по двум измерениям. Для точечной согласованности мы отобрали 130 проверяемых элементов,
эксперты независимо оценили каждый, и мы сравнили их с вердиктами агента: они совпадают
в 94% случаев, при этом расхождения сконцентрированы на субъективных критериях визуального качества, а не на
функциональных спецификациях. Для согласованности ранжирования мы оценили 8 передовых моделей (Claude Sonnet
4.5, Claude Opus 4.5, Gemini 3 Pro, GLM-4.7, DeepSeek-V3.2 и др.) используя как автоматизированную
среду, так и экспертов. Полученные ранжирования моделей достигают корреляции Спирмена
85,7%, что указывает на сильную положительную корреляцию.
Как показано в таблице 8, GLM-5 достигает 98,0% BSR и конкурентоспособна с Claude Opus 4.5 по
CSR, однако заметный разрыв в ISR сохраняется во всех трёх стеках, что указывает на то, что GLM-5 удовлетворяет большинству отдельных
требований, но всё ещё уступает Claude Opus 4.5 в выполнении всей задачи от начала до конца.
6.2.2
Оценка Backend
Оценка backend измеряет, может ли агент программирования вносить корректные, проходящие тесты модификации
в реальные серверные кодовые базы при реалистичных инженерных ограничениях. Мы составили 85 задач
охватывающих шесть языков (Python, Go, C++, Rust, Java и TypeScript), покрывающих такие области, как
поисковые системы, движки баз данных, веб-фреймворки, сервисы AI-инференса, системы управления знаниями
и отдельные алгоритмические задачи и задачи системного программирования. Типы задач включают реализацию функций,
исправление ошибок, устранение регрессий и оптимизацию производительности, отражая разнообразие
повседневной backend-разработки.
Чтобы обеспечить полностью автоматизированную оценку, каждая задача снабжена созданными экспертами unit-тестами (5–10
на задачу), которые проверяют как функциональную корректность, так и обработку граничных случаев. Задачи упакованы в
стиле terminal-bench: каждая выполняется внутри Docker-контейнера, инициализированного из фактического окружения сборки
проекта, и агент получает описание проблемы на естественном языке, описывающее требуемое
изменение. Мы сообщаем Pass@1, где задача считается решённой только если все связанные с ней unit-тесты пройдены.
Строгий критерий «всё или ничего» делает этот бенчмарк особенно сложным: GLM-5 и Claude
Opus 4.5 показывают сопоставимые результаты (таблица 8), оба значительно опережают GLM-4.7.
6.2.3
Оценка Long-horizon
Оценка long-horizon нацелена на возможности, которые отличают агентную инженерию производственного уровня
от программирования в один ход: навигацию по массивным кодовым базам и выполнение многошаговой разработки,
где каждое действие изменяет контекст для последующих. Мы разбиваем это на две взаимодополняющие
задачи.
Исследование большого репозитория. Необходимым условием для любой нетривиальной задачи программирования является способность найти
нужные исходные файлы в большом незнакомом репозитории. Мы создали автоматизированный бенчмарк на основе
реальных высокорейтинговых репозиториев GitHub, содержащих десятки тысяч файлов. Каждый вопрос сформулирован
на естественном, ориентированном на пользователя языке на уровне бизнес-семантики, строго избегая любого упоминания
имён файлов, классов или функций. Более того, вопросы требуют одного или двух переходов логического
рассуждения от описания, ориентированного на пользователя, к фактической реализации—например, вопрос
о рассинхронизации губ в сгенерированном видео соотносится с блоком настройки параметров внутри
backend генерации видео. Целевые файлы выбираются для максимального усложнения навигации: они находятся как минимум
на три уровня каталогов в глубину, имеют непрозрачные имена, устойчивые к поиску по ключевым словам, реализуют уникальную
функциональность, не дублированную в других местах репозитория, и находятся за пределами его основной функциональной поверхности.
Мы сообщаем Pass@1, усреднённый по трём прогонам, где вопрос считается решённым, если агент
успешно считывает целевой файл в ходе исследования. В этой задаче GLM-5 превосходит Claude Opus
4.5 (таблица 8), обе модели далеко опережают GLM-4.7. Результат предполагает, что эффективное исследование репозитория зависит
не столько от способности генерации кода, сколько от стратегического поиска, т.е. итеративного сужения пространства
файлов через рассуждения на уровне каталогов и семантические ассоциации, где обучение GLM-5 на агентных
траекториях использования инструментов обеспечивает явное преимущество.
26