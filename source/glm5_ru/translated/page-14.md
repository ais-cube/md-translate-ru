обучающие промпты отбираются из соответствующих наборов RL-обучения учителей и смешиваются в
соответствующих пропорциях. Функция потерь при обучении может быть получена путём замены члена преимущества в уравнении 1
следующей формулой ('sg' обозначает операцию остановки градиента, например, .detach()):
ˆAi,t = sg
"
log πinfer
θteacher(yi,t | x, yi,<t)
πtrain
θ
(yi,t | x, yi,<t)
#
.
(2)
В настоящее время мы используем механизм инференса для получения логитов учителей. В будущем мы планируем перенести
бэкенд инференса в движок обучения и единообразно применять режим Multi-Query Attention (MQA)
MLA для инференса (πinfer
θteacher →πtrain
θteacher). Во время обучения размер группы в алгоритме GRPO
устанавливается равным 1 для увеличения пропускной способности данных, а размер батча устанавливается равным 1 024. Это возможно на данном
этапе, поскольку больше нет необходимости поддерживать большую группу образцов на промпт для оценки
преимуществ; преимущество вычисляется напрямую из разрыва с моделями-учителями.
3.6
Инфраструктура RL-обучения: фреймворк slime
Мы продолжаем использовать slime в качестве унифицированной инфраструктуры пост-обучения для GLM-5, обеспечивая комплексное
обучение с подкреплением (RL) в масштабе. Вместо внедрения новых системных компонентов GLM-5
полностью использует возможности slime для (1) расширения охвата задач через настройку rollout в произвольной форме
и серверную модель выполнения, (2) существенного увеличения пропускной способности через смешанную точность
обучения/rollout вместе с MTP и разделением Prefill-Decode (PD) — в частности, для многошаговых
RL-рабочих нагрузок, и (3) повышения устойчивости через отказоустойчивость rollout на основе heartbeat и
управление жизненным циклом сервера на уровне маршрутизатора.
3.6.1
Горизонтальное масштабирование: гибкое обучение через настраиваемые Rollout
Пост-обучение GLM-5 охватывает широкий спектр задач. Для поддержки этого разнообразия без
специфичных для задач ответвлений GLM-5 использует настраиваемый интерфейс rollout slime вместе с его
серверным выполнением rollout.
Настраиваемые rollout. slime предоставляет гибкий интерфейс для реализации специфичной для задач
логики rollout — включая многошаговые циклы взаимодействия, вызов инструментов, обработку обратной связи от среды
и ветвление под управлением верификатора — без изменения базовой инфраструктуры. GLM-5 использует
эту возможность для поддержки широкого диапазона доменов и парадигм обучения, включая, помимо прочего,
RL для рассуждений, общее RL, агентное RL и дистилляцию на основе политики, всё в рамках унифицированного стека обучения.
Серверные rollout через HTTP API. slime предоставляет свои серверы rollout и маршрутизатор инференса через
стандартные HTTP API, позволяя пользователям взаимодействовать со слоем обслуживания slime так же, как с
обычным движком инференса. Это отделяет логику rollout от границы процесса обучения:
внешние агентные фреймворки и среды могут вызывать конечные точки сервера/маршрутизатора напрямую, в то время как
бэкенд оптимизации остаётся неизменным как для краткосрочного одношагового обучения, так и для долгосрочных
многошаговых траекторий.
3.6.2
Вертикальное масштабирование: оптимизация хвостовой задержки для RL Rollout
Для RL rollout целью оптимизации является не агрегированная пропускная способность, а комплексная задержка, определяемая
самым медленным (хвостовым) образцом на каждом шаге. На практике одна отстающая траектория может
остановить точки синхронизации (например, завершение батча, готовность буфера, обновления тренера) и напрямую
определять временной прогресс. Поэтому GLM-5 полностью использует ориентированные на задержку механизмы обслуживания и
планирования slime для минимизации как медианной задержки, так и, что более важно, хвостовой задержки.
Обслуживание без очередей через многоузловой инференс с DP-attention для MLA. Чтобы избежать
задержек из-за очередей, запросы rollout должны обслуживаться оперативно даже при пакетном трафике, что требует значительной
ёмкости KV-cache. GLM-5 использует развёртывание многоузлового инференса (например, EP64 и DP64 на 8
узлах) для обеспечения достаточного распределённого KV-cache. DP-attention в первую очередь внедрён для предотвращения
копирования KV между различными рангами.
Снижение хвостовой задержки с помощью FP8 rollout и MTP. GLM-5 использует FP8 для инференса rollout для
уменьшения задержки на токен и сокращения времени выполнения длинных траекторий. Кроме того, GLM-5
использует поддержку slime для Multi-Token Prediction (MTP), что особенно эффективно в режиме декодирования с малым батчем, типичном для RL rollout. Поскольку хвостовая задержка часто определяется малым BS
14