Файл: page-16

предопределенного порога, пакет отправляется движку обучения для обновления модели. Чтобы уменьшить отставание политики и поддерживать обучение приблизительно on-policy, веса модели, используемые движком rollout, периодически синхронизируются с весами движка обучения. Движок обучения обновляет параметры модели и отправляет новые веса обратно движку инференса каждые K обновлений градиента.
Хотя асинхронность может значительно повысить общую эффективность обучения, она также означает, что различные траектории могут генерироваться разными версиями модели, что создает серьезную проблему off-policy.
Поскольку обновление весов учитывает иную задачу оптимизации из-за изменения политики rollout,
мы также сбрасываем оптимизатор после каждого обновления весов движка инференса.
Серверная архитектура многозадачного обучения.
Для решения проблемы гетерогенности генерации траекторий
в многозадачном RL, где различные задачи обычно используют различные наборы инструментов и специфичную для задач логику rollout, мы представляем серверный многозадачный оркестратор rollout для многозадачного обучения RL. Этот
компонент обеспечивает бесшовную совместимость между фреймворком обучения slime RL
и различными downstream-задачами через центральный оркестратор с несколькими зарегистрированными сервисами задач.
В частности, каждая задача реализует собственную логику rollout и вознаграждения как независимый микросервис,
который регистрируется в центральном оркестраторе для управления и планирования. На стадии rollout центральный оркестратор контролирует соотношение rollout для каждой задачи и скорость генерации для достижения
сбалансированного сбора данных по задачам. Главное, мы стандартизируем траектории из всех агентных задач
в единое представление в виде списка сообщений. Это позволяет совместно обучать сложные агентные фреймворки
(например, задачу программной инженерии), а также поддерживать централизованную постобработку и логирование для
гетерогенных рабочих нагрузок. Эта архитектура четко изолирует специфичную для задач логику от основного цикла обучения,
обеспечивая бесшовную интеграцию с многозадачным обучением RL. Будучи основой инфраструктуры обучения GLM-5, этот оркестратор поддерживает более 1 000 одновременных rollout и обеспечивает автоматическую динамическую настройку соотношений выборки задач, а также детальный мониторинг прогресса задач.
4.1.2
Оптимизация стабильности асинхронного обучения
Token-in-Token-out против Text-in-Text-out.
В контексте RL rollout token-in-token-out (TITO)
означает, что конвейер обучения потребляет точную токенизацию и поток декодированных токенов, произведенный
движком инференса, и использует его непосредственно для построения траекторий для обучения. В отличие от этого, text-in-text-out
рассматривает движок rollout как черный ящик, возвращающий финальный текст; обучающая система затем реконструирует траекторию путем повторной токенизации этого текста (и часто повторного определения границ и усечения) перед вычислением
потерь. Этот кажущийся незначительным выбор имеет серьезные последствия: повторная токенизация может привести к тонким несоответствиям
в границах токенов, обработке пробелов/нормализации, усечении или размещении специальных токенов,
что, в свою очередь, может нарушить выравнивание шагов между действиями и вознаграждениями/преимуществами — особенно когда
rollout потоковые, усеченные или чередуются между множеством акторов. Мы обнаружили, что token-in-token-out
критически важен для асинхронного обучения RL, поскольку он сохраняет точное соответствие на уровне действий между
тем, что было выбрано, и тем, что оптимизируется, при этом позволяя акторам немедленно выдавать фрагменты траекторий (идентификаторы токенов + метаданные) без потерь при преобразовании в текст и обратно и без ожидания постфактум
повторной токенизации на стороне обучающей системы. На практике мы реализуем TITO Gateway, который перехватывает все
запросы генерации от задач rollout и записывает идентификаторы токенов и метаданные каждой траектории. Эта
архитектура изолирует громоздкую обработку идентификаторов токенов от нижележащей логики rollout агента, избегая при этом несоответствий повторной токенизации во время обучения RL.
Прямая двусторонняя importance sampling для отсечения токенов.
В отличие от синхронной настройки обучения RL
в разделе 3, в асинхронной настройке движки rollout могут проходить через несколько обновлений
в течение одной генерации траектории, что делает отслеживание точных вероятностей поведения
πθold вычислительно запретительным. В противном случае пришлось бы поддерживать обширную историю контрольных точек модели {πθ(1)
old , . . . , πθ(N)
old }, что неосуществимо в практической реализации.
Для решения этой проблемы мы, во-первых, применяем упрощенный механизм importance sampling на уровне токенов, который повторно использует
лог-вероятности, сгенерированные во время rollout, как прямой прокси поведения. Рассчитывая соотношение importance sampling как rt(θ) =
πθ
πrollout и отбрасывая традиционную πθold, мы устраняем вычислительные
издержки отдельного инференса старой политики. Во-вторых, мы применяем стратегию двусторонней калибровочной маскировки на уровне токенов. Вместо асимметричного отсечения, используемого в стандартном PPO, мы ограничиваем область доверия
интервалом [1 −ϵℓ, 1 + ϵh], где ϵℓ и ϵh — гиперпараметры отсечения. Токены, выходящие за пределы этого интервала,
полностью маскируются при вычислении градиента для предотвращения нестабильностей, вызванных экстремальным расхождением политики. Это имеет сходство с механизмом IcePop [44], однако наша стратегия проще за счет
дополнительного удаления πθold и достижения более стабильного обучения.
16