Таблица 9: Производительность на SWE-rebench, январь 2026.
Модель
Уровень решения (%)
СКО уровня решения (±, %)
Pass@5 (%)
Claude Opus 4.6
52,9%
1,06%
70,8%
GPT-5.2 (xhigh)
51,7%
1,21%
58,3%
Claude Sonnet 4.5
47,1%
1,69%
60,4%
Gemini 3 Pro
46,7%
2,04%
58,3%
Claude Opus 4.5
43,8%
0,93%
58,3%
GLM-5
42,1%
1,21%
50,0%
GLM-4.7
41,3%
2,12%
56,3%
Kimi K2.5
37,9%
1,21%
50,0%
Многошаговые последовательные задачи. Распространённые тесты кодирования, такие как SWE-bench, сводят оценку
к изолированным правкам в одном коммите и поэтому не могут оценить способность агента выполнять инкрементальную
разработку, где каждый шаг изменяет состояние кодовой базы для последующих шагов. Чтобы решить эту проблему, мы
строим тест с длительным горизонтом, извлекая объединённые Pull Request из высококачественных репозиториев
и собирая цепочки задач через следующий конвейер:
1. Фильтрация PR. Сохраняются только объединённые PR, которые включают тесты, содержат 3–15 коммитов и следуют
линейной (без слияний) истории.
2. Семантическая группировка. LLM оценивает попарную семантическую связанность между смежными коммитами;
динамическое программирование находит оптимальное разбиение на связные группы задач, которое максимизирует
внутригрупповую связность, сохраняя при этом порядок коммитов.
3. Сортировка патчей. Кумулятивный diff каждой задачи разделяется на три категории: эталонный патч (основной код,
который должен создать агент), тестовый патч (проверочные тесты) и автоприменяемый патч (конфигурация и
фикстуры, применяемые автоматически).
4. Генерация формулировки задачи. LLM генерирует формулировку задачи на естественном языке для
каждой задачи на основе её патча и сообщений коммитов.
5. Классификация задач. Задачи автоматически классифицируются (функция / исправление ошибки / рефакторинг / тест / конфигурация)
и оцениваются по трём осям: устранение ошибок, точность критического пути и прохождение тестов.
6. Валидация окружения. Создаются Docker-окружения, и эталонные патчи применяются
для проверки отсутствия регрессий по всей цепочке.

Для цепочки из K задач агент начинает с базового коммита и работает последовательно: после
завершения задачи k её изменения фиксируются коммитом, и применяется автоприменяемый патч для задачи k+1, так что
состояние кодовой базы эволюционирует кумулятивно. Оценка проверяет каждый коммит по очереди и кумулятивно
применяет тестовые патчи от задач 1 до k перед запуском полного набора тестов, выявляя как сбои
в текущей задаче, так и регрессии в предыдущих. Мы приводим Pass@1 для отдельных задач. Такая
последовательная и рекурсивная по состоянию конструкция напрямую оценивает отслеживание контекста на большом расстоянии, планирование и
способности инкрементальной разработки, которые не проверяются тестами с одним коммитом. Как показывает Таблица 8,
GLM-5 существенно улучшается по сравнению с GLM-4.7, но значительный разрыв с Claude Opus 4.5 остаётся. Это
происходит потому, что ошибки накапливаются по цепочке: неоптимальная правка в одной задаче может незаметно нарушить
тесты в последующих задачах. Сокращение этого разрыва потребует достижений в долгосрочной согласованности контекста и
долгосрочной самокоррекции — обеих активных направлениях наших текущих исследований.

6.2.4
Оценка на развивающихся SWE-задачах

Мы проводим оценку на SWE-rebench [4], потому что SWE-bench Verified — это статический, публичный, валидированный людьми
тестовый набор, выпущенный более 2 лет назад. В отличие от него, SWE-rebench построен на автоматизированном
конвейере, который непрерывно извлекает свежие, реальные задачи по исправлению проблем GitHub, обеспечивая деконтаминированную,
устойчивую во времени оценку, которая лучше измеряет обобщение на новые задачи программной инженерии,
а не производительность на статическом тесте. Таблица 9 показывает официальную производительность GLM-5 на
SWE-rebench, и мы наблюдаем, что GLM-5 может эффективно обобщаться на новые SWE-задачи.

27