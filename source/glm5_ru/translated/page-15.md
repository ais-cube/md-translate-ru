отстающих (например, редкие длинные контексты, сложные многошаговые рассуждения, трассировки с интенсивным использованием инструментов), MTP обеспечивает непропорционально большие преимущества на длинном хвосте, улучшая время до завершения самого медленного
образца и, таким образом, сокращая время простоя на уровне шагов.
Разделение PD для предотвращения взаимных помех prefill-decode в многошаговом RL. В многошаговых сценариях
часто встречаются prefill с длинными префиксами (история диалога, трассировки инструментов, контекст кода). При DP-attention
совместное размещение prefill и decode на одних и тех же ресурсах обслуживания может создать серьёзные помехи: тяжёлый
prefill может вытеснить или нарушить выполняющиеся decode на сервере, не позволяя другим образцам добиваться
непрерывного прогресса и резко ухудшая хвостовую задержку. GLM-5, следовательно, использует разделение Prefill–
Decode (PD) в slime. Запуская prefill и decode на выделенных ресурсах, decode остаются
стабильными и непрерывными, позволяя образцам с длинным горизонтом прогрессировать непрерывно и значительно
улучшая хвостовое поведение в многошаговом агентном RL.
3.6.3
Устойчивость Rollout: отказоустойчивость на основе heartbeat
В масштабе неизбежны временные сбои (например, сбои отдельных серверов, проблемы с сетью или деградация производительности).
GLM-5 использует отказоустойчивость на основе heartbeat в slime для обеспечения непрерывности обучения
при таких событиях: серверы rollout периодически отправляют heartbeat, которые отслеживаются слоем оркестрации,
и неработоспособные серверы проактивно завершаются и удаляются из маршрутизатора inference.
В результате повторные попытки автоматически перенаправляются от отказавших или деградировавших серверов к работоспособным,
предотвращая прерывание rollout из-за инцидентов на отдельных серверах и сохраняя непрерывное сквозное
обучение RL.
4
Агентная инженерия
Мы описываем переход от vibe coding (человеческие промпты) к агентной инженерии. В vibe
coding человек отправляет промпты модели ИИ для написания кода. В агентной инженерии агенты ИИ пишут
код самостоятельно. Они планируют, реализуют и итерируют. Для поддержки этих задач с длинным горизонтом GLM-5
использует полностью асинхронную и разделённую инфраструктуру RL для значительного повышения утилизации GPU
за счёт сокращения времени простоя во время rollout агентов. Для масштабирования агентных окружений мы разработали
конвейеры построения окружений. Для задач кодирования мы настраиваем задачи реальной разработки ПО
и терминальные задачи, создавая более 10 000 верифицируемых сценариев обучения. Для агентов поиска мы разработали
автоматический и масштабируемый конвейер синтеза данных сложных многошаговых рассуждений для построения данных для
агентного обучения.
4.1
Асинхронное RL для агентных задач
Для проведения RL для агентных задач мы разработали полностью асинхронную и разделённую инфраструктуру RL, которая
эффективно обрабатывает rollout агентов с длинным горизонтом и поддерживает гибкое многозадачное обучение RL на
различных агентных фреймворках.
Мы применяем алгоритм групповой оптимизации политики для обучения RL. Для каждой задачи x мы
сэмплируем K трассировок агента {y1, . . . , yK} из предыдущей политики πold и оптимизируем модель πθ относительно
следующей целевой функции:
L(θ) = Ex∼D
"
1
K
K
X
i=1
(r(x, yi) −¯r(x))
#
,
где ¯r(x) =
1
K
PK
i=1 r
 x, yi

— средняя награда сэмплированных ответов. Следует отметить, что только
токены, сгенерированные моделью, используются для оптимизации, а обратная связь от окружения игнорируется при вычислении loss.
4.1.1
Дизайн асинхронного RL для агентного обучения
Из-за длиннохвостового характера процесса rollout наивное синхронное обучение RL создаёт значительные
простои на этапе rollout из-за сильно несбалансированной генерации агентных задач,
что может вызвать большое время простоя GPU. Для повышения пропускной способности обучения мы применяем полностью асинхронную
парадигму обучения для Agentic RL для повышения утилизации GPU и эффективности обучения. Конкретно, мы
разделяем движок обучения и движок inference на разные устройства GPU. Движок inference
непрерывно генерирует траектории. Когда количество сгенерированных траекторий достигает
15