Таблица 1: Результаты оценки для GQA-8 и вариантов MLA.
Набор данных
Hellaswag
MMLU
C-Eval
RACE
BBH
GSM8K
HumanEval
GQA-8
77,3
61,2
60,0
79,6
53,3
47,6
38,5
MLA
77,3
61,5
59,7
77,8
48,9
46,2
33,5
MLA + Muon Split
77,8
62,5
62,1
79,9
51,8
45,0
36,7
MLA-256 + Muon Split
77,4
62,0
59,9
79,6
51,3
47,5
36,6
dimension KV-cache). Для преодоления разрыва в производительности мы предлагаем адаптацию рецепта оптимизатора Muon в GLM-4.5. В исходном рецепте мы применяем матричную ортогонализацию к матрицам повышающей проекции W UQ, W UK, W UV для многоголовочных запросов, ключей и значений. Вместо этого мы разделяем эти матрицы на меньшие матрицы для разных голов и применяем матричную ортогонализацию к этим независимым матрицам. Метод, обозначенный как Muon Split, позволяет весам проекций для разных голов внимания обновляться с разными масштабами. Как показано в таблице 1, метод эффективно улучшает производительность MLA до уровня GQA-8. На практике мы также обнаруживаем, что с Muon Split масштаб логитов внимания GLM-5 остаётся стабильным во время предобучения без какой-либо стратегии отсечения.

Другим недостатком MLA является высокая вычислительная стоимость во время декодирования. При декодировании MLA выполняет скалярное произведение размерности 576, что выше, чем 128-мерное вычисление GQA. Хотя количество голов внимания в DeepSeek-V3 выбрано в соответствии с roofline H800 [60], оно неприемлемо для другого оборудования. Учитывая стиль Multi-head Attention (MHA) в MLA во время обучения и prefilling, мы увеличиваем размерность головы с 192 до 256 и уменьшаем количество голов внимания на 1/3. Это сохраняет вычисления при обучении и количество параметров постоянными, уменьшая при этом вычисления при декодировании. Вариант, обозначенный как MLA-256 в таблице 1, соответствует производительности MLA при использовании Muon Split.

Таблица 2: Сравнение длин принятия
DeepSeek-V3.2 и GLM-5.
Модель
Длина принятия
DeepSeek-V3.2
2,55
GLM-5
2,76
Предсказание нескольких токенов с совместным использованием параметров.
Предсказание нескольких токенов (MTP) [13; 25] повышает производительность базовых моделей и выступает в качестве черновых моделей для спекулятивного декодирования [20]. Однако во время обучения для предсказания следующих n токенов требуется n слоёв MTP. В результате использование памяти параметрами MTP и kv cache масштабируется линейно с количеством спекулятивных шагов. Вместо этого DeepSeek-V3 обучается с одним слоем MTP и предсказывает следующие 2 токена во время вывода. Расхождение между обучением и выводом снижает коэффициент принятия второго токена. Поэтому мы предлагаем совместное использование параметров 3 слоёв MTP во время обучения. Это сохраняет затраты памяти черновой модели на уровне DeepSeek-V3, одновременно увеличивая коэффициент принятия. В таблице 2 мы показываем, что длина принятия GLM-5 больше, чем у DeepSeek-V3.2, при одинаковом количестве спекулятивных шагов (4) в нашем приватном наборе промптов.

2.1.1
Продолжение предобучения с DeepSeek Sparse Attention (DSA)

Таблица 3: Сравнение бенчмарков длинного контекста между базовыми моделями MLA и DSA.
MQ-NIAH-128k
MV-NIAH-128k
SQuAD-128k
HotpotQA-128k
MLA
100,0
95,5
79,7
66,3
DSA
100,0
97,0
86,0
63,0

Мы используем DSA в нашем обучении. Основная философия DSA [9] заключается в замене традиционного плотного внимания O(L2), которое становится чрезмерно затратным при контекстах 128K, динамическим механизмом детальной выборки. В отличие от фиксированных паттернов (таких как скользящие окна), DSA «смотрит» на содержимое, чтобы решить, какие токены важны. Что делает DSA особенно интересным с точки зрения исследователя — это способ его внедрения через продолжение предобучения от плотной базовой модели. Это позволило избежать «астрономических» затрат на обучение с нуля. Переход следует двухэтапной стратегии «плотный прогрев и адаптация разреженного обучения». DeepSeek-V3.2-Exp поддерживает