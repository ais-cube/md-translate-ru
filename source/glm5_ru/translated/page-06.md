0,0
0,2
0,4
0,6
0,8
1,0
Шаг
0,35
0,40
0,45
0,50
0,55
Потери
MLA
DSA
0,66
0,68
0,70
0,72
0,74
0,76
0,78
0,0004
0,0002
0,0000
0,0002
0,0004
Относительные потери
Рисунок 6: Сравнение кривых потерь SFT между обучением MLA и DSA. Результаты сглажены
скользящим средним с размером окна 50.
ту же производительность на бенчмарках, что и его плотный предшественник, доказывая, что 90% элементов внимания в
длинных контекстах действительно избыточны. DSA сокращает вычисления внимания примерно в 1,5–2× для
длинных последовательностей, что крайне важно для агентов с интенсивными рассуждениями, которые мы создаём, позволяя
обрабатывать контексты длиной 128K при вдвое меньших затратах GPU.

Обучение DSA начинается с базовой модели в конце промежуточного обучения. Этап прогрева проходит
через 1 000 шагов, каждый шаг обучается на 14 последовательностях по 202 752 токена с максимальной
скоростью обучения 5e-3. Этап разреженной адаптации следует обучающим данным и гиперпараметрам
промежуточного обучения и проходит через 20 млрд токенов. Хотя бюджет обучения намного меньше, чем у
DeepSeek-V3.2 (943,7 млрд токенов), мы обнаружили, что его достаточно для адаптации модели DSA к достижению
производительности исходной модели MLA. Как показано в таблице 3, производительность модели
DSA на длинных контекстах близка к производительности модели MLA. Для дальнейшей проверки эффективности обучения DSA
мы выполняем тонкую настройку моделей DSA и MLA на одних и тех же данных SFT соответственно и обнаруживаем, что обе
модели показывают одинаковые потери при обучении и результаты на бенчмарках для оценки.

2.1.2 Исследование эффективных вариантов внимания методом исключения

Помимо DSA [26], мы исследуем несколько альтернативных механизмов эффективного внимания на основе GLM-9B1.
Базовая модель использует групповое внимание с запросами во всех 40 слоях и была дообучена с
окном контекста в 128K токенов. Мы оцениваем следующие подходы:

• Чередование скользящего оконного внимания (SWA): Фиксированный чередующийся паттерн слоёв с полным вниманием и
оконным вниманием, применяемый равномерно по всей сети.
• Gated DeltaNet (GDN) [54]: Вариант линейного внимания, заменяющий квадратичное вычисление внимания с softmax
на линейную рекуррентность с вентилями, сокращая вычислительную стоимость внимания
с квадратичной до линейной относительно длины последовательности.

Основываясь на этих базовых подходах, мы предлагаем два улучшения:

• Паттерн SWA (на основе поиска): Вдохновлённые PostNAS [15], мы вводим метод адаптации
на основе поиска, который идентифицирует оптимальное подмножество слоёв для преобразования в SWA с сохранением
полного внимания в остальных слоях. Мы используем стратегию лучевого поиска для определения
конфигурации, максимизирующей производительность на задачах с длинным контекстом. Для снижения
вычислительных затрат мы проводим поиск исключительно на длине контекста 16K и обобщаем
полученный паттерн на все остальные длины входных данных. В частности, мы используем размер луча 8, оптимизируя
два слоя на шаг; для GLM-9B (40 слоёв) процесс сходится примерно за 10 шагов.
На каждом шаге паттерны-кандидаты оцениваются на бенчмарке RULER [17] при длине контекста 16K,
и топ-8 кандидатов сохраняются для следующего шага. Итоговый выведенный паттерн:
SFSSFFSSSFFFFSSFSFFFFFFSFSFSSFSSFSFSSFSSS, где S и F обозначают слои SWA и полного
внимания соответственно. Как показано в таблице 4, эта конфигурация на основе поиска значительно
превосходит фиксированный чередующийся подход. Примечательно, что несмотря на оптимизацию только для 16K,

1Одна из наших моделей серии GLM-4, доступная по адресу https://github.com/zai-org/GLM-4
6