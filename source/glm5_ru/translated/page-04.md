Рисунок 5: Общий конвейер обучения GLM-5.

В-третьих, мы представляем новые асинхронные алгоритмы обучения с подкреплением агентов, разработанные для повышения качества автономного принятия решений. В GLM-4.5 мы использовали итеративную самодистилляцию и контроль результатов для обучения агентов. Для GLM-5 мы разработали асинхронные алгоритмы, которые позволяют модели непрерывно учиться на разнообразных взаимодействиях с длительным горизонтом. Эти алгоритмы специально оптимизированы для улучшения способностей модели к планированию и самокоррекции в динамических средах, что напрямую способствует нашему доминированию в реальных сценариях программирования.

Наконец, ещё один технический вклад заключается в том, что с первого дня GLM-5 полностью адаптирована к китайским GPU-экосистемам. Мы успешно завершили глубокую оптимизацию — от базовых ядер до высокоуровневых фреймворков вывода — на семи основных отечественных чиповых платформах, включая Huawei Ascend, Moore Threads, Hygon, Cambricon, Kunlunxin, MetaX и Enflame.

Благодаря этим достижениям GLM-5 представляет собой не просто более мощную модель, но и более эффективную и практичную основу для следующего поколения ИИ-агентов. Мы выпускаем GLM-5 для сообщества, чтобы продвинуть границы эффективного агентного общего интеллекта.

2
Предобучение

Аналогично GLM-4.5, базовая модель GLM-5 проходит два этапа: предобучение для общих языковых способностей и программирования, и промежуточное обучение для агентных способностей и обработки длинного контекста. Мы расширили бюджет токенов обучения для всех этапов обучения GLM-5, в общей сложности 28,5 триллиона токенов для базовой модели.

2.1
Архитектура

Масштабирование размера модели.
GLM-5 масштабируется до 256 экспертов и сокращает количество слоёв до 80 для минимизации накладных расходов на обмен данными при параллелизме экспертов. В результате получается модель с 744 миллиардами параметров (40 миллиардов активных параметров), что вдвое превышает общий размер GLM-4.5, которая использовала 355 миллиардов общих и 32 миллиарда активных параметров.

Многолатентное внимание.
За счёт использования сокращённых векторов ключей-значений многолатентное внимание (MLA) [24] соответствует эффективности группового внимания с запросами (GQA), но обеспечивает превосходную экономию памяти GPU и более быструю обработку для последовательностей с длинным контекстом.

Однако в наших экспериментах с оптимизатором Muon мы обнаружили, что MLA с 576-мерным латентным KV-кэшем не может сравниться с производительностью GQA с 8 группами запросов (обозначается как GQA-8, 2048-